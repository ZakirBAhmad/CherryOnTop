{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import src.load as load\n",
    "import src.utils as utils\n",
    "from src.model import KiloModel,ScheduleModel,FinalModel\n",
    "from src.encoder import ClimateEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.utils.weight_norm as weight_norm\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_year, test_year, _, _, _ = load.separate_year('../data/processed/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prop, test_prop, _, _, _ = load.separate_prop('../data/processed/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_prop, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(test_prop, batch_size=32, shuffle=True)\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_lr = 1e-4  # Reduced from 1e-3 to prevent NaN\n",
    "s_lr = 1e-4\n",
    "f_lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakir/anaconda3/envs/cherry/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.GRU):\n",
    "        for param in m.parameters():\n",
    "            if len(param.shape) >= 2:\n",
    "                torch.nn.init.orthogonal_(param.data)\n",
    "            else:\n",
    "                torch.nn.init.normal_(param.data)\n",
    "\n",
    "k_encoder = ClimateEncoder()\n",
    "k_model = KiloModel(k_encoder)\n",
    "\n",
    "# Initialize weights for better stability\n",
    "k_model.apply(init_weights)\n",
    "\n",
    "k_opt = torch.optim.Adam(k_model.parameters(), lr=k_lr,weight_decay=1e-4)\n",
    "\n",
    "k_scheduler = torch.optim.lr_scheduler.StepLR(k_opt, step_size=10, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_weeks = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_kilo_model(model,train_loader,val_loader,criterion,optimizer,scheduler,num_epochs,num_weeks):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for batch in train_loader:\n",
    "            features, encoded_features, climate_data, y_kilos, y_combined, schedule, _= batch\n",
    "            \n",
    "            # Check for NaN in input data\n",
    "            if torch.isnan(features).any() or torch.isnan(climate_data).any() or torch.isnan(y_combined).any():\n",
    "                print(\"NaN detected in input data, skipping batch\")\n",
    "                continue\n",
    "                \n",
    "            y = y_kilos.cumsum(dim=1)\n",
    "            climate_data = climate_data[:,:num_weeks * 7,:]\n",
    "            inputs = y_combined[:,:num_weeks,:]\n",
    "            outputs = model(features, encoded_features, climate_data, inputs).cumsum(dim=1)\n",
    "            loss = 0.5 * criterion(outputs, y) + criterion(outputs[:,num_weeks:], y[:,num_weeks:])\n",
    "\n",
    "            # Check for NaN in loss\n",
    "            if torch.isnan(loss):\n",
    "                print(f\"NaN loss detected at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                features, encoded_features, climate_data, y_kilos, y_combined, schedule, _= batch\n",
    "                y = y_kilos.cumsum(dim=1)\n",
    "                climate_data = climate_data[:,:num_weeks * 7,:]\n",
    "                inputs = y_combined[:,:num_weeks,:]\n",
    "                outputs = model(features, encoded_features, climate_data, inputs).cumsum(dim=1)\n",
    "                loss = criterion(outputs,y)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        # Check for NaN in validation loss\n",
    "        avg_train_loss = train_loss/len(train_loader)\n",
    "        avg_val_loss = val_loss/len(val_loader)\n",
    "        \n",
    "        if torch.isnan(torch.tensor(avg_train_loss)) or torch.isnan(torch.tensor(avg_val_loss)):\n",
    "            print(f\"NaN detected at epoch {epoch+1}. Stopping training.\")\n",
    "            break\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Week {num_weeks}, Train Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zakir/anaconda3/envs/cherry/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s_encoder = ClimateEncoder()\n",
    "s_model = ScheduleModel(s_encoder)\n",
    "s_opt = torch.optim.Adam(s_model.parameters(), lr=s_lr,weight_decay=1e-4)\n",
    "s_scheduler = torch.optim.lr_scheduler.StepLR(s_opt, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_schedule_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, num_weeks):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for batch in train_loader:\n",
    "            features, encoded_features, climate_data, _, y_combined, schedule, _ = batch\n",
    "            \n",
    "            # Check for NaN in input data\n",
    "            if torch.isnan(features).any() or torch.isnan(climate_data).any() or torch.isnan(schedule).any():\n",
    "                print(\"NaN detected in input data, skipping batch\")\n",
    "                continue\n",
    "                \n",
    "            climate_data = climate_data[:, :num_weeks * 7, :]\n",
    "            inputs = y_combined[:, :num_weeks, :]\n",
    "            outputs = model(features, encoded_features, climate_data, inputs)\n",
    "            loss = criterion(outputs, schedule)\n",
    "\n",
    "            # Check for NaN in loss\n",
    "            if torch.isnan(loss):\n",
    "                print(f\"NaN loss detected at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                features, encoded_features, climate_data, _, y_combined, schedule, _ = batch\n",
    "                climate_data = climate_data[:, :num_weeks * 7, :]\n",
    "                inputs = y_combined[:, :num_weeks, :]\n",
    "                outputs = model(features, encoded_features, climate_data, inputs)\n",
    "                loss = criterion(outputs, schedule)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "        # Check for NaN in validation loss\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        if torch.isnan(torch.tensor(avg_train_loss)) or torch.isnan(torch.tensor(avg_val_loss)):\n",
    "            print(f\"NaN detected at epoch {epoch+1}. Stopping training.\")\n",
    "            break\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Week {num_weeks}, Train Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Week 1, Train Loss: 78211949.53684211, Validation Loss: 52176134.166666664\n",
      "Epoch 2/50, Week 1, Train Loss: 77862495.28421053, Validation Loss: 52009259.75\n",
      "Epoch 3/50, Week 1, Train Loss: 77479433.07368422, Validation Loss: 51700149.75\n",
      "Epoch 4/50, Week 1, Train Loss: 76822959.97894737, Validation Loss: 51708737.416666664\n",
      "Epoch 5/50, Week 1, Train Loss: 76258443.38947369, Validation Loss: 50881944.125\n",
      "Epoch 6/50, Week 1, Train Loss: 75403154.94736843, Validation Loss: 50144028.833333336\n",
      "Epoch 7/50, Week 1, Train Loss: 74297466.21052632, Validation Loss: 49783385.083333336\n",
      "Epoch 8/50, Week 1, Train Loss: 73068048.05263157, Validation Loss: 48893858.333333336\n",
      "Epoch 9/50, Week 1, Train Loss: 71895232.35789473, Validation Loss: 47677357.958333336\n",
      "Epoch 10/50, Week 1, Train Loss: 70913223.26315789, Validation Loss: 46735642.291666664\n",
      "Epoch 11/50, Week 1, Train Loss: 70052671.2, Validation Loss: 46366102.333333336\n",
      "Epoch 12/50, Week 1, Train Loss: 69981952.29473685, Validation Loss: 46495621.833333336\n",
      "Epoch 13/50, Week 1, Train Loss: 69990402.64210527, Validation Loss: 46363200.625\n",
      "Epoch 14/50, Week 1, Train Loss: 69599890.24210526, Validation Loss: 46714166.25\n",
      "Epoch 15/50, Week 1, Train Loss: 69650065.5263158, Validation Loss: 46541389.75\n",
      "Epoch 16/50, Week 1, Train Loss: 69583418.25263157, Validation Loss: 45983665.208333336\n",
      "Epoch 17/50, Week 1, Train Loss: 69290857.70526315, Validation Loss: 45909884.583333336\n",
      "Epoch 18/50, Week 1, Train Loss: 69430302.41052632, Validation Loss: 46260061.75\n",
      "Epoch 19/50, Week 1, Train Loss: 69152222.71578947, Validation Loss: 45977050.5\n",
      "Epoch 20/50, Week 1, Train Loss: 69306894.65263158, Validation Loss: 45925386.25\n",
      "Epoch 21/50, Week 1, Train Loss: 68994448.08421053, Validation Loss: 46427728.854166664\n",
      "Epoch 22/50, Week 1, Train Loss: 69037639.41052632, Validation Loss: 45903709.875\n",
      "Epoch 23/50, Week 1, Train Loss: 69044513.74736843, Validation Loss: 46072211.0\n",
      "Epoch 24/50, Week 1, Train Loss: 68888099.09473684, Validation Loss: 45853873.75\n",
      "Epoch 25/50, Week 1, Train Loss: 68920624.71578947, Validation Loss: 46202588.208333336\n",
      "Epoch 26/50, Week 1, Train Loss: 68905587.02105263, Validation Loss: 46102959.958333336\n",
      "Epoch 27/50, Week 1, Train Loss: 69012697.51578948, Validation Loss: 46023714.875\n",
      "Epoch 28/50, Week 1, Train Loss: 69000889.01052631, Validation Loss: 46349272.666666664\n",
      "Epoch 29/50, Week 1, Train Loss: 68953066.0, Validation Loss: 45942864.541666664\n",
      "Epoch 30/50, Week 1, Train Loss: 69030926.63157895, Validation Loss: 45617745.333333336\n",
      "Epoch 31/50, Week 1, Train Loss: 68856861.76842105, Validation Loss: 45850902.583333336\n",
      "Epoch 32/50, Week 1, Train Loss: 68972372.29473685, Validation Loss: 45881886.833333336\n",
      "Epoch 33/50, Week 1, Train Loss: 68853187.62105264, Validation Loss: 45917738.916666664\n",
      "Epoch 34/50, Week 1, Train Loss: 69094299.05263157, Validation Loss: 46154327.333333336\n",
      "Epoch 35/50, Week 1, Train Loss: 69009976.26315789, Validation Loss: 45743130.166666664\n",
      "Epoch 36/50, Week 1, Train Loss: 68934168.24210526, Validation Loss: 45854495.416666664\n",
      "Epoch 37/50, Week 1, Train Loss: 68836886.61052631, Validation Loss: 45680182.25\n",
      "Epoch 38/50, Week 1, Train Loss: 68961523.73684211, Validation Loss: 46044505.916666664\n",
      "Epoch 39/50, Week 1, Train Loss: 68985900.87894736, Validation Loss: 45831235.458333336\n",
      "Epoch 40/50, Week 1, Train Loss: 68879691.63157895, Validation Loss: 46508948.333333336\n",
      "Epoch 41/50, Week 1, Train Loss: 68928391.36842105, Validation Loss: 46308682.5\n",
      "Epoch 42/50, Week 1, Train Loss: 68858610.48421052, Validation Loss: 45585592.916666664\n",
      "Epoch 43/50, Week 1, Train Loss: 68881453.64210527, Validation Loss: 46179379.666666664\n",
      "Epoch 44/50, Week 1, Train Loss: 68888434.87368421, Validation Loss: 45841146.166666664\n",
      "Epoch 45/50, Week 1, Train Loss: 69018316.85263158, Validation Loss: 45974251.75\n",
      "Epoch 46/50, Week 1, Train Loss: 68901678.16842106, Validation Loss: 46078704.75\n",
      "Epoch 47/50, Week 1, Train Loss: 68859120.32631579, Validation Loss: 45841944.166666664\n",
      "Epoch 48/50, Week 1, Train Loss: 68827035.62105264, Validation Loss: 45582649.583333336\n",
      "Epoch 49/50, Week 1, Train Loss: 68876899.17894737, Validation Loss: 46433304.333333336\n",
      "Epoch 50/50, Week 1, Train Loss: 69009696.82105263, Validation Loss: 45999745.166666664\n",
      "Epoch 1/50, Week 1, Train Loss: 157.87934554250617, Validation Loss: 155.19703483581543\n",
      "Epoch 2/50, Week 1, Train Loss: 140.22061631052117, Validation Loss: 133.06871350606283\n",
      "Epoch 3/50, Week 1, Train Loss: 113.05821244089228, Validation Loss: 100.36219755808513\n",
      "Epoch 4/50, Week 1, Train Loss: 79.18831995913857, Validation Loss: 65.91155290603638\n",
      "Epoch 5/50, Week 1, Train Loss: 47.88463686892861, Validation Loss: 37.49286127090454\n",
      "Epoch 6/50, Week 1, Train Loss: 24.878445334183542, Validation Loss: 19.518003861109417\n",
      "Epoch 7/50, Week 1, Train Loss: 13.433602162411338, Validation Loss: 12.332921902338663\n",
      "Epoch 8/50, Week 1, Train Loss: 9.615903989892256, Validation Loss: 9.670629700024923\n",
      "Epoch 9/50, Week 1, Train Loss: 8.014654204719944, Validation Loss: 8.330212593078613\n",
      "Epoch 10/50, Week 1, Train Loss: 7.077800103237754, Validation Loss: 7.4402896364529925\n",
      "Epoch 11/50, Week 1, Train Loss: 6.71586389541626, Validation Loss: 7.375374436378479\n",
      "Epoch 12/50, Week 1, Train Loss: 6.670108780108, Validation Loss: 7.346024831136067\n",
      "Epoch 13/50, Week 1, Train Loss: 6.625262561597322, Validation Loss: 7.253596285978953\n",
      "Epoch 14/50, Week 1, Train Loss: 6.581769797676488, Validation Loss: 7.190276384353638\n",
      "Epoch 15/50, Week 1, Train Loss: 6.543600870433607, Validation Loss: 7.182610551516215\n",
      "Epoch 16/50, Week 1, Train Loss: 6.498885295265599, Validation Loss: 7.10640283425649\n",
      "Epoch 17/50, Week 1, Train Loss: 6.461152726725528, Validation Loss: 7.119405090808868\n",
      "Epoch 18/50, Week 1, Train Loss: 6.437197409178081, Validation Loss: 7.04772412776947\n",
      "Epoch 19/50, Week 1, Train Loss: 6.396300275702226, Validation Loss: 6.995000580946605\n",
      "Epoch 20/50, Week 1, Train Loss: 6.361631237833124, Validation Loss: 6.929292281468709\n",
      "Epoch 21/50, Week 1, Train Loss: 6.3427613233265125, Validation Loss: 6.926653325557709\n",
      "Epoch 22/50, Week 1, Train Loss: 6.339360279785959, Validation Loss: 6.960402925809224\n",
      "Epoch 23/50, Week 1, Train Loss: 6.334816691749975, Validation Loss: 6.944038887818654\n",
      "Epoch 24/50, Week 1, Train Loss: 6.332695449026008, Validation Loss: 6.919318318367004\n",
      "Epoch 25/50, Week 1, Train Loss: 6.3216692698629275, Validation Loss: 6.938948253790538\n",
      "Epoch 26/50, Week 1, Train Loss: 6.324000589471114, Validation Loss: 6.928763012091319\n",
      "Epoch 27/50, Week 1, Train Loss: 6.3275691433956744, Validation Loss: 6.92722209294637\n",
      "Epoch 28/50, Week 1, Train Loss: 6.324403574592189, Validation Loss: 6.914674123128255\n",
      "Epoch 29/50, Week 1, Train Loss: 6.3159849668803965, Validation Loss: 6.959772109985352\n",
      "Epoch 30/50, Week 1, Train Loss: 6.309451655337685, Validation Loss: 6.924440324306488\n",
      "Epoch 31/50, Week 1, Train Loss: 6.314601097608867, Validation Loss: 6.906644304593404\n",
      "Epoch 32/50, Week 1, Train Loss: 6.309423263449418, Validation Loss: 6.90985894203186\n",
      "Epoch 33/50, Week 1, Train Loss: 6.314261496694464, Validation Loss: 6.926689128081004\n",
      "Epoch 34/50, Week 1, Train Loss: 6.311564784300955, Validation Loss: 6.925621390342712\n",
      "Epoch 35/50, Week 1, Train Loss: 6.307935433638724, Validation Loss: 6.946350157260895\n",
      "Epoch 36/50, Week 1, Train Loss: 6.306928413792661, Validation Loss: 6.934826234976451\n",
      "Epoch 37/50, Week 1, Train Loss: 6.310632361863789, Validation Loss: 6.976006011168162\n",
      "Epoch 38/50, Week 1, Train Loss: 6.306289190995066, Validation Loss: 6.921353379885356\n",
      "Epoch 39/50, Week 1, Train Loss: 6.308321132157978, Validation Loss: 6.947014272212982\n",
      "Epoch 40/50, Week 1, Train Loss: 6.3127672747561805, Validation Loss: 6.951971530914307\n",
      "Epoch 41/50, Week 1, Train Loss: 6.313594592245002, Validation Loss: 6.906528155008952\n",
      "Epoch 42/50, Week 1, Train Loss: 6.312726984525982, Validation Loss: 6.914663553237915\n",
      "Epoch 43/50, Week 1, Train Loss: 6.309996810712312, Validation Loss: 6.901957372824351\n",
      "Epoch 44/50, Week 1, Train Loss: 6.317951701816759, Validation Loss: 6.906055053075154\n",
      "Epoch 45/50, Week 1, Train Loss: 6.305744386974134, Validation Loss: 6.887369791666667\n",
      "Epoch 46/50, Week 1, Train Loss: 6.308214729710629, Validation Loss: 6.871535440286\n",
      "Epoch 47/50, Week 1, Train Loss: 6.312656708767539, Validation Loss: 6.883868137995402\n",
      "Epoch 48/50, Week 1, Train Loss: 6.3063484643634995, Validation Loss: 6.886776248613994\n",
      "Epoch 49/50, Week 1, Train Loss: 6.311740413464998, Validation Loss: 6.883127709229787\n",
      "Epoch 50/50, Week 1, Train Loss: 6.310083138315301, Validation Loss: 6.912109086910884\n",
      "Epoch 1/50, Week 10, Train Loss: 82926642.21052632, Validation Loss: 46320942.25\n",
      "Epoch 2/50, Week 10, Train Loss: 82754294.56842105, Validation Loss: 46179379.166666664\n",
      "Epoch 3/50, Week 10, Train Loss: 82605026.81052631, Validation Loss: 45558756.958333336\n",
      "Epoch 4/50, Week 10, Train Loss: 82827195.24210526, Validation Loss: 46121903.416666664\n",
      "Epoch 5/50, Week 10, Train Loss: 82643800.42105263, Validation Loss: 45699596.5\n",
      "Epoch 6/50, Week 10, Train Loss: 82715004.34210527, Validation Loss: 46160333.625\n",
      "Epoch 7/50, Week 10, Train Loss: 82781077.35789473, Validation Loss: 45917316.791666664\n",
      "Epoch 8/50, Week 10, Train Loss: 82722879.56842105, Validation Loss: 45658074.666666664\n",
      "Epoch 9/50, Week 10, Train Loss: 82686352.18947369, Validation Loss: 45927295.291666664\n",
      "Epoch 10/50, Week 10, Train Loss: 82687437.38947369, Validation Loss: 45431049.958333336\n",
      "Epoch 11/50, Week 10, Train Loss: 82772513.53684211, Validation Loss: 45735474.583333336\n",
      "Epoch 12/50, Week 10, Train Loss: 82690099.41052632, Validation Loss: 46066187.625\n",
      "Epoch 13/50, Week 10, Train Loss: 82699120.98947369, Validation Loss: 45733258.875\n",
      "Epoch 14/50, Week 10, Train Loss: 82872680.19473684, Validation Loss: 45505164.020833336\n",
      "Epoch 15/50, Week 10, Train Loss: 82718966.21052632, Validation Loss: 45849106.416666664\n",
      "Epoch 16/50, Week 10, Train Loss: 82618885.8631579, Validation Loss: 45947820.666666664\n",
      "Epoch 17/50, Week 10, Train Loss: 82699636.82631579, Validation Loss: 46241849.25\n",
      "Epoch 18/50, Week 10, Train Loss: 82715051.74736843, Validation Loss: 46253368.625\n",
      "Epoch 19/50, Week 10, Train Loss: 82754584.08421053, Validation Loss: 45783542.75\n",
      "Epoch 20/50, Week 10, Train Loss: 82874814.65263158, Validation Loss: 45814547.104166664\n",
      "Epoch 21/50, Week 10, Train Loss: 82545281.14736842, Validation Loss: 46049217.833333336\n",
      "Epoch 22/50, Week 10, Train Loss: 82735385.81052631, Validation Loss: 46338166.208333336\n",
      "Epoch 23/50, Week 10, Train Loss: 82616051.71578947, Validation Loss: 46072831.333333336\n",
      "Epoch 24/50, Week 10, Train Loss: 82564511.38947369, Validation Loss: 45658716.25\n",
      "Epoch 25/50, Week 10, Train Loss: 82660000.73684211, Validation Loss: 45970694.0\n",
      "Epoch 26/50, Week 10, Train Loss: 82605414.43157895, Validation Loss: 46178972.291666664\n",
      "Epoch 27/50, Week 10, Train Loss: 82612152.74736843, Validation Loss: 45641387.041666664\n",
      "Epoch 28/50, Week 10, Train Loss: 82725706.50526316, Validation Loss: 46125493.083333336\n",
      "Epoch 29/50, Week 10, Train Loss: 82590236.90526316, Validation Loss: 46409882.0\n",
      "Epoch 30/50, Week 10, Train Loss: 82816915.16842106, Validation Loss: 45612060.416666664\n",
      "Epoch 31/50, Week 10, Train Loss: 82739507.31578948, Validation Loss: 45783097.0\n",
      "Epoch 32/50, Week 10, Train Loss: 82699086.14736842, Validation Loss: 46129543.583333336\n",
      "Epoch 33/50, Week 10, Train Loss: 82771435.53684211, Validation Loss: 45684656.0\n",
      "Epoch 34/50, Week 10, Train Loss: 83007974.67368421, Validation Loss: 45935353.0\n",
      "Epoch 35/50, Week 10, Train Loss: 82937827.22105263, Validation Loss: 46376583.666666664\n",
      "Epoch 36/50, Week 10, Train Loss: 82883771.57894737, Validation Loss: 45735786.625\n",
      "Epoch 37/50, Week 10, Train Loss: 82823550.88421053, Validation Loss: 45755757.166666664\n",
      "Epoch 38/50, Week 10, Train Loss: 82682652.77894737, Validation Loss: 45853598.0\n",
      "Epoch 39/50, Week 10, Train Loss: 82946200.43157895, Validation Loss: 45772626.416666664\n",
      "Epoch 40/50, Week 10, Train Loss: 82801517.30526316, Validation Loss: 46829773.708333336\n",
      "Epoch 41/50, Week 10, Train Loss: 82697177.07368422, Validation Loss: 45672641.895833336\n",
      "Epoch 42/50, Week 10, Train Loss: 82629129.11578947, Validation Loss: 45625131.916666664\n",
      "Epoch 43/50, Week 10, Train Loss: 82663250.16842106, Validation Loss: 45818959.166666664\n",
      "Epoch 44/50, Week 10, Train Loss: 82705005.58947368, Validation Loss: 45746123.0\n",
      "Epoch 45/50, Week 10, Train Loss: 82700653.10526316, Validation Loss: 46132889.625\n",
      "Epoch 46/50, Week 10, Train Loss: 82754995.58947368, Validation Loss: 46311795.083333336\n",
      "Epoch 47/50, Week 10, Train Loss: 82940787.05263157, Validation Loss: 46148458.166666664\n",
      "Epoch 48/50, Week 10, Train Loss: 82889985.30526316, Validation Loss: 45860082.5\n",
      "Epoch 49/50, Week 10, Train Loss: 82809270.8631579, Validation Loss: 45954612.333333336\n",
      "Epoch 50/50, Week 10, Train Loss: 82650284.08421053, Validation Loss: 46002970.083333336\n",
      "Epoch 1/50, Week 10, Train Loss: 6.235326566194233, Validation Loss: 6.664678951104482\n",
      "Epoch 2/50, Week 10, Train Loss: 6.232245568225259, Validation Loss: 6.671902060508728\n",
      "Epoch 3/50, Week 10, Train Loss: 6.230765884800961, Validation Loss: 6.673295418421428\n",
      "Epoch 4/50, Week 10, Train Loss: 6.232345796886244, Validation Loss: 6.691828827063243\n",
      "Epoch 5/50, Week 10, Train Loss: 6.22981908697831, Validation Loss: 6.696660578250885\n",
      "Epoch 6/50, Week 10, Train Loss: 6.231509120840776, Validation Loss: 6.6762169400850935\n",
      "Epoch 7/50, Week 10, Train Loss: 6.228738970505564, Validation Loss: 6.663047234217326\n",
      "Epoch 8/50, Week 10, Train Loss: 6.238333616758648, Validation Loss: 6.679469168186188\n",
      "Epoch 9/50, Week 10, Train Loss: 6.231976348475406, Validation Loss: 6.650927881399791\n",
      "Epoch 10/50, Week 10, Train Loss: 6.237765159105, Validation Loss: 6.6657959421475725\n",
      "Epoch 11/50, Week 10, Train Loss: 6.2253321095516805, Validation Loss: 6.682264447212219\n",
      "Epoch 12/50, Week 10, Train Loss: 6.231789834875809, Validation Loss: 6.70715715487798\n",
      "Epoch 13/50, Week 10, Train Loss: 6.234052655571386, Validation Loss: 6.662502825260162\n",
      "Epoch 14/50, Week 10, Train Loss: 6.233013489371852, Validation Loss: 6.662816822528839\n",
      "Epoch 15/50, Week 10, Train Loss: 6.2326694312848545, Validation Loss: 6.681842625141144\n",
      "Epoch 16/50, Week 10, Train Loss: 6.229929738295706, Validation Loss: 6.665605922540029\n",
      "Epoch 17/50, Week 10, Train Loss: 6.2306437893917685, Validation Loss: 6.665825585524241\n",
      "Epoch 18/50, Week 10, Train Loss: 6.237084338539525, Validation Loss: 6.6755204399426775\n",
      "Epoch 19/50, Week 10, Train Loss: 6.233585417898078, Validation Loss: 6.672966122627258\n",
      "Epoch 20/50, Week 10, Train Loss: 6.234844092318887, Validation Loss: 6.6548890471458435\n",
      "Epoch 21/50, Week 10, Train Loss: 6.234150863948622, Validation Loss: 6.650571842988332\n",
      "Epoch 22/50, Week 10, Train Loss: 6.229581476512708, Validation Loss: 6.69657576084137\n",
      "Epoch 23/50, Week 10, Train Loss: 6.229940359215988, Validation Loss: 6.675757944583893\n",
      "Epoch 24/50, Week 10, Train Loss: 6.2275457884136, Validation Loss: 6.672937273979187\n",
      "Epoch 25/50, Week 10, Train Loss: 6.226336027446546, Validation Loss: 6.680845538775126\n",
      "Epoch 26/50, Week 10, Train Loss: 6.236633948275918, Validation Loss: 6.688485999902089\n",
      "Epoch 27/50, Week 10, Train Loss: 6.225789880752563, Validation Loss: 6.70567403237025\n",
      "Epoch 28/50, Week 10, Train Loss: 6.238365840911865, Validation Loss: 6.6871296763420105\n",
      "Epoch 29/50, Week 10, Train Loss: 6.231393914473684, Validation Loss: 6.685334463914235\n",
      "Epoch 30/50, Week 10, Train Loss: 6.235841404764276, Validation Loss: 6.691919366518657\n",
      "Epoch 31/50, Week 10, Train Loss: 6.225653116326583, Validation Loss: 6.696133971214294\n",
      "Epoch 32/50, Week 10, Train Loss: 6.233512557180304, Validation Loss: 6.6390663882096606\n",
      "Epoch 33/50, Week 10, Train Loss: 6.229821702053672, Validation Loss: 6.684699098269145\n",
      "Epoch 34/50, Week 10, Train Loss: 6.232033418354235, Validation Loss: 6.675997018814087\n",
      "Epoch 35/50, Week 10, Train Loss: 6.229439763018959, Validation Loss: 6.684161404768626\n",
      "Epoch 36/50, Week 10, Train Loss: 6.228665903994911, Validation Loss: 6.672854761282603\n",
      "Epoch 37/50, Week 10, Train Loss: 6.232194877925672, Validation Loss: 6.6930757363637285\n",
      "Epoch 38/50, Week 10, Train Loss: 6.230808368482088, Validation Loss: 6.663537581761678\n",
      "Epoch 39/50, Week 10, Train Loss: 6.230183947713751, Validation Loss: 6.67553323507309\n",
      "Epoch 40/50, Week 10, Train Loss: 6.231464975758603, Validation Loss: 6.717910647392273\n",
      "Epoch 41/50, Week 10, Train Loss: 6.235417381085847, Validation Loss: 6.6654051542282104\n",
      "Epoch 42/50, Week 10, Train Loss: 6.226891748528732, Validation Loss: 6.659034887949626\n",
      "Epoch 43/50, Week 10, Train Loss: 6.229903266304418, Validation Loss: 6.677703936894734\n",
      "Epoch 44/50, Week 10, Train Loss: 6.231775484587017, Validation Loss: 6.659804066022237\n",
      "Epoch 45/50, Week 10, Train Loss: 6.225385071101941, Validation Loss: 6.701274414857228\n",
      "Epoch 46/50, Week 10, Train Loss: 6.2319176347632155, Validation Loss: 6.677201767762502\n",
      "Epoch 47/50, Week 10, Train Loss: 6.231010685468975, Validation Loss: 6.662896851698558\n",
      "Epoch 48/50, Week 10, Train Loss: 6.23108360892848, Validation Loss: 6.712416330973308\n",
      "Epoch 49/50, Week 10, Train Loss: 6.229495746211001, Validation Loss: 6.69088613986969\n",
      "Epoch 50/50, Week 10, Train Loss: 6.227409415496023, Validation Loss: 6.6463435888290405\n",
      "Epoch 1/20, Week 14, Train Loss: 91382334.17894737, Validation Loss: 46195046.833333336\n",
      "Epoch 2/20, Week 14, Train Loss: 91384342.90526316, Validation Loss: 45781469.791666664\n",
      "Epoch 3/20, Week 14, Train Loss: 91340418.5263158, Validation Loss: 46026080.833333336\n",
      "Epoch 4/20, Week 14, Train Loss: 91344910.42105263, Validation Loss: 45769599.083333336\n",
      "Epoch 5/20, Week 14, Train Loss: 91465748.48421052, Validation Loss: 46579066.416666664\n",
      "Epoch 6/20, Week 14, Train Loss: 91652561.24210526, Validation Loss: 46021557.333333336\n",
      "Epoch 7/20, Week 14, Train Loss: 91530407.07368422, Validation Loss: 45859833.75\n",
      "Epoch 8/20, Week 14, Train Loss: 91404642.58947368, Validation Loss: 45683537.333333336\n",
      "Epoch 9/20, Week 14, Train Loss: 91580273.7263158, Validation Loss: 46175486.166666664\n",
      "Epoch 10/20, Week 14, Train Loss: 91581262.88421053, Validation Loss: 46158669.416666664\n",
      "Epoch 11/20, Week 14, Train Loss: 91392739.1368421, Validation Loss: 45861293.083333336\n",
      "Epoch 12/20, Week 14, Train Loss: 91762243.03157894, Validation Loss: 46485716.541666664\n",
      "Epoch 13/20, Week 14, Train Loss: 91564982.37894736, Validation Loss: 46043941.166666664\n",
      "Epoch 14/20, Week 14, Train Loss: 91587822.73684211, Validation Loss: 45696100.5\n",
      "Epoch 15/20, Week 14, Train Loss: 91517309.1368421, Validation Loss: 45918933.395833336\n",
      "Epoch 16/20, Week 14, Train Loss: 91431572.14736842, Validation Loss: 45555991.583333336\n",
      "Epoch 17/20, Week 14, Train Loss: 91399715.98947369, Validation Loss: 46672526.916666664\n",
      "Epoch 18/20, Week 14, Train Loss: 91512896.63157895, Validation Loss: 46107716.541666664\n",
      "Epoch 19/20, Week 14, Train Loss: 91432843.81052631, Validation Loss: 45659277.416666664\n",
      "Epoch 20/20, Week 14, Train Loss: 91507711.9368421, Validation Loss: 46169522.0\n",
      "Epoch 1/20, Week 14, Train Loss: 6.237754440307617, Validation Loss: 6.67887145280838\n",
      "Epoch 2/20, Week 14, Train Loss: 6.235229005311664, Validation Loss: 6.654392917950948\n",
      "Epoch 3/20, Week 14, Train Loss: 6.211899205258018, Validation Loss: 6.6717122594515486\n",
      "Epoch 4/20, Week 14, Train Loss: 6.213502326764559, Validation Loss: 6.66843185822169\n",
      "Epoch 5/20, Week 14, Train Loss: 6.225446118806538, Validation Loss: 6.702887296676636\n",
      "Epoch 6/20, Week 14, Train Loss: 6.2281960136012025, Validation Loss: 6.679333567619324\n",
      "Epoch 7/20, Week 14, Train Loss: 6.22235813642803, Validation Loss: 6.659328103065491\n",
      "Epoch 8/20, Week 14, Train Loss: 6.2123762155834, Validation Loss: 6.708707670370738\n",
      "Epoch 9/20, Week 14, Train Loss: 6.224712396922865, Validation Loss: 6.667384584744771\n",
      "Epoch 10/20, Week 14, Train Loss: 6.219272623564068, Validation Loss: 6.711610198020935\n",
      "Epoch 11/20, Week 14, Train Loss: 6.2352268570347835, Validation Loss: 6.692205190658569\n",
      "Epoch 12/20, Week 14, Train Loss: 6.2321701350965, Validation Loss: 6.689351856708527\n",
      "Epoch 13/20, Week 14, Train Loss: 6.219267328161942, Validation Loss: 6.670310616493225\n",
      "Epoch 14/20, Week 14, Train Loss: 6.216994769949662, Validation Loss: 6.69231375058492\n",
      "Epoch 15/20, Week 14, Train Loss: 6.2200217698749745, Validation Loss: 6.686496218045552\n",
      "Epoch 16/20, Week 14, Train Loss: 6.219259114014475, Validation Loss: 6.670491894086202\n",
      "Epoch 17/20, Week 14, Train Loss: 6.221740160490337, Validation Loss: 6.650724331537883\n",
      "Epoch 18/20, Week 14, Train Loss: 6.214950579091123, Validation Loss: 6.675269385178884\n",
      "Epoch 19/20, Week 14, Train Loss: 6.217313896982294, Validation Loss: 6.698322256406148\n",
      "Epoch 20/20, Week 14, Train Loss: 6.224417455572831, Validation Loss: 6.688185612360637\n",
      "Epoch 1/10, Week 20, Train Loss: 100148555.83157894, Validation Loss: 46011636.416666664\n",
      "Epoch 2/10, Week 20, Train Loss: 100186620.69473684, Validation Loss: 45608476.833333336\n",
      "Epoch 3/10, Week 20, Train Loss: 99962419.70526315, Validation Loss: 46227708.333333336\n",
      "Epoch 4/10, Week 20, Train Loss: 99903245.11578947, Validation Loss: 45497938.291666664\n",
      "Epoch 5/10, Week 20, Train Loss: 100105096.92631578, Validation Loss: 46409917.666666664\n",
      "Epoch 6/10, Week 20, Train Loss: 100131071.45263158, Validation Loss: 46325958.791666664\n",
      "Epoch 7/10, Week 20, Train Loss: 99851098.8, Validation Loss: 45983584.083333336\n",
      "Epoch 8/10, Week 20, Train Loss: 100021670.44210526, Validation Loss: 45991380.875\n",
      "Epoch 9/10, Week 20, Train Loss: 99902344.58947368, Validation Loss: 45924104.583333336\n",
      "Epoch 10/10, Week 20, Train Loss: 99896355.74736843, Validation Loss: 46302887.583333336\n",
      "Epoch 1/10, Week 20, Train Loss: 6.252604795757093, Validation Loss: 6.716191748778026\n",
      "Epoch 2/10, Week 20, Train Loss: 6.252811095589085, Validation Loss: 6.715486367543538\n",
      "Epoch 3/10, Week 20, Train Loss: 6.244569841184114, Validation Loss: 6.713332871596019\n",
      "Epoch 4/10, Week 20, Train Loss: 6.250451346447593, Validation Loss: 6.7172767122586565\n",
      "Epoch 5/10, Week 20, Train Loss: 6.252161156503778, Validation Loss: 6.707514464855194\n",
      "Epoch 6/10, Week 20, Train Loss: 6.2453293800354, Validation Loss: 6.7038081884384155\n",
      "Epoch 7/10, Week 20, Train Loss: 6.249242328342638, Validation Loss: 6.750900387763977\n",
      "Epoch 8/10, Week 20, Train Loss: 6.253892542186536, Validation Loss: 6.718644718329112\n",
      "Epoch 9/10, Week 20, Train Loss: 6.24952668139809, Validation Loss: 6.74833349386851\n",
      "Epoch 10/10, Week 20, Train Loss: 6.250463555988513, Validation Loss: 6.712326566378276\n",
      "Epoch 1/5, Week 25, Train Loss: 100597967.44210526, Validation Loss: 45673329.25\n",
      "Epoch 2/5, Week 25, Train Loss: 100379962.12631579, Validation Loss: 46091765.333333336\n",
      "Epoch 3/5, Week 25, Train Loss: 100479495.15789473, Validation Loss: 46004575.958333336\n",
      "Epoch 4/5, Week 25, Train Loss: 100492537.57894737, Validation Loss: 45532209.916666664\n",
      "Epoch 5/5, Week 25, Train Loss: 100629962.8631579, Validation Loss: 45631626.708333336\n",
      "Epoch 1/5, Week 25, Train Loss: 6.22570561860737, Validation Loss: 6.719510316848755\n",
      "Epoch 2/5, Week 25, Train Loss: 6.219013688438817, Validation Loss: 6.701636711756389\n",
      "Epoch 3/5, Week 25, Train Loss: 6.218386602401734, Validation Loss: 6.69743796189626\n",
      "Epoch 4/5, Week 25, Train Loss: 6.222978737479762, Validation Loss: 6.700726687908173\n",
      "Epoch 5/5, Week 25, Train Loss: 6.228421065681859, Validation Loss: 6.726819574832916\n",
      "Epoch 1/20, Week 14, Train Loss: 91604456.29473685, Validation Loss: 46079549.041666664\n",
      "Epoch 2/20, Week 14, Train Loss: 91591393.57894737, Validation Loss: 45861413.916666664\n",
      "Epoch 3/20, Week 14, Train Loss: 91596766.90526316, Validation Loss: 47039544.416666664\n",
      "Epoch 4/20, Week 14, Train Loss: 91502183.85263158, Validation Loss: 45826094.875\n",
      "Epoch 5/20, Week 14, Train Loss: 91558025.05263157, Validation Loss: 45716443.5\n",
      "Epoch 6/20, Week 14, Train Loss: 91480061.43157895, Validation Loss: 46565100.5\n",
      "Epoch 7/20, Week 14, Train Loss: 91487685.81052631, Validation Loss: 45631148.104166664\n",
      "Epoch 8/20, Week 14, Train Loss: 91420358.0, Validation Loss: 45515468.916666664\n",
      "Epoch 9/20, Week 14, Train Loss: 91462386.10526316, Validation Loss: 45730158.166666664\n",
      "Epoch 10/20, Week 14, Train Loss: 91733863.53684211, Validation Loss: 45815961.666666664\n",
      "Epoch 11/20, Week 14, Train Loss: 91411308.73684211, Validation Loss: 45631432.666666664\n",
      "Epoch 12/20, Week 14, Train Loss: 91480971.3368421, Validation Loss: 45557965.416666664\n",
      "Epoch 13/20, Week 14, Train Loss: 91434678.54736842, Validation Loss: 45606057.291666664\n",
      "Epoch 14/20, Week 14, Train Loss: 91474786.15, Validation Loss: 46127882.958333336\n",
      "Epoch 15/20, Week 14, Train Loss: 91363396.04210526, Validation Loss: 45724939.916666664\n",
      "Epoch 16/20, Week 14, Train Loss: 91731602.77894737, Validation Loss: 45837622.916666664\n",
      "Epoch 17/20, Week 14, Train Loss: 91558027.4736842, Validation Loss: 46257434.416666664\n",
      "Epoch 18/20, Week 14, Train Loss: 91875709.74736843, Validation Loss: 46014027.416666664\n",
      "Epoch 19/20, Week 14, Train Loss: 91384472.38947369, Validation Loss: 46011593.791666664\n",
      "Epoch 20/20, Week 14, Train Loss: 91406607.43157895, Validation Loss: 46299367.666666664\n",
      "Epoch 1/20, Week 14, Train Loss: 6.221502123380962, Validation Loss: 6.665868580341339\n",
      "Epoch 2/20, Week 14, Train Loss: 6.218174236699155, Validation Loss: 6.685238142808278\n",
      "Epoch 3/20, Week 14, Train Loss: 6.225021944547954, Validation Loss: 6.672055562337239\n",
      "Epoch 4/20, Week 14, Train Loss: 6.235125315816779, Validation Loss: 6.7117405732472735\n",
      "Epoch 5/20, Week 14, Train Loss: 6.225185419383802, Validation Loss: 6.671458919843038\n",
      "Epoch 6/20, Week 14, Train Loss: 6.23305485875983, Validation Loss: 6.659396409988403\n",
      "Epoch 7/20, Week 14, Train Loss: 6.219199963619834, Validation Loss: 6.657328565915425\n",
      "Epoch 8/20, Week 14, Train Loss: 6.226523258811549, Validation Loss: 6.695233364899953\n",
      "Epoch 9/20, Week 14, Train Loss: 6.220101968865645, Validation Loss: 6.656829675038655\n",
      "Epoch 10/20, Week 14, Train Loss: 6.22352045962685, Validation Loss: 6.6916143496831255\n",
      "Epoch 11/20, Week 14, Train Loss: 6.221702071240074, Validation Loss: 6.655932784080505\n",
      "Epoch 12/20, Week 14, Train Loss: 6.230306447179694, Validation Loss: 6.702523390452067\n",
      "Epoch 13/20, Week 14, Train Loss: 6.231625657332571, Validation Loss: 6.670586665471395\n",
      "Epoch 14/20, Week 14, Train Loss: 6.223607336847405, Validation Loss: 6.685917516549428\n",
      "Epoch 15/20, Week 14, Train Loss: 6.229688922982467, Validation Loss: 6.666682660579681\n",
      "Epoch 16/20, Week 14, Train Loss: 6.228783853430497, Validation Loss: 6.666254639625549\n",
      "Epoch 17/20, Week 14, Train Loss: 6.220651967901933, Validation Loss: 6.66709824403127\n",
      "Epoch 18/20, Week 14, Train Loss: 6.222704814609728, Validation Loss: 6.649935722351074\n",
      "Epoch 19/20, Week 14, Train Loss: 6.219029461710076, Validation Loss: 6.67157514890035\n",
      "Epoch 20/20, Week 14, Train Loss: 6.223800169794183, Validation Loss: 6.68903785943985\n",
      "Epoch 1/50, Week 10, Train Loss: 82764211.22105263, Validation Loss: 45685281.5\n",
      "Epoch 2/50, Week 10, Train Loss: 82705670.2736842, Validation Loss: 46021244.666666664\n",
      "Epoch 3/50, Week 10, Train Loss: 82843880.2736842, Validation Loss: 45938771.958333336\n",
      "Epoch 4/50, Week 10, Train Loss: 82611163.26315789, Validation Loss: 45667379.125\n",
      "Epoch 5/50, Week 10, Train Loss: 82856073.4736842, Validation Loss: 45904391.5\n",
      "Epoch 6/50, Week 10, Train Loss: 82606373.01052631, Validation Loss: 45986656.375\n",
      "Epoch 7/50, Week 10, Train Loss: 82653668.91578947, Validation Loss: 45859726.416666664\n",
      "Epoch 8/50, Week 10, Train Loss: 82746358.32631579, Validation Loss: 45692621.666666664\n",
      "Epoch 9/50, Week 10, Train Loss: 82617285.22105263, Validation Loss: 45905090.666666664\n",
      "Epoch 10/50, Week 10, Train Loss: 82787152.31578948, Validation Loss: 45932625.958333336\n",
      "Epoch 11/50, Week 10, Train Loss: 82758816.61052631, Validation Loss: 45727788.541666664\n",
      "Epoch 12/50, Week 10, Train Loss: 82628647.69473684, Validation Loss: 46178891.5\n",
      "Epoch 13/50, Week 10, Train Loss: 82557474.98947369, Validation Loss: 45543905.0\n",
      "Epoch 14/50, Week 10, Train Loss: 82704939.07368422, Validation Loss: 45421623.833333336\n",
      "Epoch 15/50, Week 10, Train Loss: 82809270.02105263, Validation Loss: 45548646.375\n",
      "Epoch 16/50, Week 10, Train Loss: 82555780.23157895, Validation Loss: 45462502.166666664\n",
      "Epoch 17/50, Week 10, Train Loss: 82684171.64210527, Validation Loss: 45722241.625\n",
      "Epoch 18/50, Week 10, Train Loss: 82755723.11578947, Validation Loss: 45527366.208333336\n",
      "Epoch 19/50, Week 10, Train Loss: 82633708.61052631, Validation Loss: 45516236.416666664\n",
      "Epoch 20/50, Week 10, Train Loss: 82699280.4736842, Validation Loss: 45635351.333333336\n",
      "Epoch 21/50, Week 10, Train Loss: 82732143.75789474, Validation Loss: 45593459.25\n",
      "Epoch 22/50, Week 10, Train Loss: 82739083.02105263, Validation Loss: 45657737.625\n",
      "Epoch 23/50, Week 10, Train Loss: 82685874.29473685, Validation Loss: 46088297.791666664\n",
      "Epoch 24/50, Week 10, Train Loss: 82768472.65263158, Validation Loss: 45612935.5\n",
      "Epoch 25/50, Week 10, Train Loss: 82577528.2, Validation Loss: 46609454.333333336\n",
      "Epoch 26/50, Week 10, Train Loss: 82662658.50526316, Validation Loss: 45751689.291666664\n",
      "Epoch 27/50, Week 10, Train Loss: 82608126.92631578, Validation Loss: 46135935.25\n",
      "Epoch 28/50, Week 10, Train Loss: 82779803.55789474, Validation Loss: 45391794.666666664\n",
      "Epoch 29/50, Week 10, Train Loss: 82609328.11578947, Validation Loss: 46065094.666666664\n",
      "Epoch 30/50, Week 10, Train Loss: 82737095.6631579, Validation Loss: 46097721.291666664\n",
      "Epoch 31/50, Week 10, Train Loss: 82730395.83157894, Validation Loss: 45724797.833333336\n",
      "Epoch 32/50, Week 10, Train Loss: 82773406.96842106, Validation Loss: 45558288.666666664\n",
      "Epoch 33/50, Week 10, Train Loss: 82672000.88421053, Validation Loss: 45914638.083333336\n",
      "Epoch 34/50, Week 10, Train Loss: 82740442.09473684, Validation Loss: 46419220.0\n",
      "Epoch 35/50, Week 10, Train Loss: 82531155.42105263, Validation Loss: 45957869.166666664\n",
      "Epoch 36/50, Week 10, Train Loss: 82743786.54736842, Validation Loss: 47242924.875\n",
      "Epoch 37/50, Week 10, Train Loss: 82692811.74736843, Validation Loss: 46036178.958333336\n",
      "Epoch 38/50, Week 10, Train Loss: 82708648.67368421, Validation Loss: 45618091.666666664\n",
      "Epoch 39/50, Week 10, Train Loss: 82606461.03157894, Validation Loss: 45545445.541666664\n",
      "Epoch 40/50, Week 10, Train Loss: 82756962.18947369, Validation Loss: 45515420.291666664\n",
      "Epoch 41/50, Week 10, Train Loss: 82656469.32631579, Validation Loss: 46083264.5\n",
      "Epoch 42/50, Week 10, Train Loss: 82589729.30526316, Validation Loss: 46285798.333333336\n",
      "Epoch 43/50, Week 10, Train Loss: 82661538.07368422, Validation Loss: 45973718.333333336\n",
      "Epoch 44/50, Week 10, Train Loss: 82711385.85263158, Validation Loss: 46043230.083333336\n",
      "Epoch 45/50, Week 10, Train Loss: 82720606.10526316, Validation Loss: 45562971.666666664\n",
      "Epoch 46/50, Week 10, Train Loss: 82732837.89473684, Validation Loss: 45889164.958333336\n",
      "Epoch 47/50, Week 10, Train Loss: 82754309.89473684, Validation Loss: 46831615.75\n",
      "Epoch 48/50, Week 10, Train Loss: 82703883.18947369, Validation Loss: 45563923.833333336\n",
      "Epoch 49/50, Week 10, Train Loss: 82744145.57894737, Validation Loss: 45575469.666666664\n",
      "Epoch 50/50, Week 10, Train Loss: 82661939.76842105, Validation Loss: 45612808.416666664\n",
      "Epoch 1/50, Week 10, Train Loss: 6.228385272778962, Validation Loss: 6.67467725276947\n",
      "Epoch 2/50, Week 10, Train Loss: 6.233234922509444, Validation Loss: 6.711782197157542\n",
      "Epoch 3/50, Week 10, Train Loss: 6.229266929626465, Validation Loss: 6.68661908308665\n",
      "Epoch 4/50, Week 10, Train Loss: 6.2435331896731725, Validation Loss: 6.663894216219584\n",
      "Epoch 5/50, Week 10, Train Loss: 6.22874137225904, Validation Loss: 6.6452537178993225\n",
      "Epoch 6/50, Week 10, Train Loss: 6.232243967056275, Validation Loss: 6.682981769243876\n",
      "Epoch 7/50, Week 10, Train Loss: 6.2250791926133004, Validation Loss: 6.665359735488892\n",
      "Epoch 8/50, Week 10, Train Loss: 6.23244561647114, Validation Loss: 6.67200227578481\n",
      "Epoch 9/50, Week 10, Train Loss: 6.226716493305407, Validation Loss: 6.6718148191769915\n",
      "Epoch 10/50, Week 10, Train Loss: 6.232339096069336, Validation Loss: 6.697370688120524\n",
      "Epoch 11/50, Week 10, Train Loss: 6.233962357671637, Validation Loss: 6.645459274450938\n",
      "Epoch 12/50, Week 10, Train Loss: 6.229310828761051, Validation Loss: 6.679819802443187\n",
      "Epoch 13/50, Week 10, Train Loss: 6.226833996019866, Validation Loss: 6.632810513178508\n",
      "Epoch 14/50, Week 10, Train Loss: 6.230954285671836, Validation Loss: 6.667513112227122\n",
      "Epoch 15/50, Week 10, Train Loss: 6.226675706160696, Validation Loss: 6.678038318951924\n",
      "Epoch 16/50, Week 10, Train Loss: 6.234794270364862, Validation Loss: 6.712018450101216\n",
      "Epoch 17/50, Week 10, Train Loss: 6.236875478844894, Validation Loss: 6.678264995416005\n",
      "Epoch 18/50, Week 10, Train Loss: 6.232254045887998, Validation Loss: 6.671217501163483\n",
      "Epoch 19/50, Week 10, Train Loss: 6.229595174287494, Validation Loss: 6.704851746559143\n",
      "Epoch 20/50, Week 10, Train Loss: 6.236112968545211, Validation Loss: 6.660544653733571\n",
      "Epoch 21/50, Week 10, Train Loss: 6.230622986743325, Validation Loss: 6.678813258806865\n",
      "Epoch 22/50, Week 10, Train Loss: 6.226571921298379, Validation Loss: 6.682160993417104\n",
      "Epoch 23/50, Week 10, Train Loss: 6.228486595655743, Validation Loss: 6.66008585691452\n",
      "Epoch 24/50, Week 10, Train Loss: 6.224672879670796, Validation Loss: 6.685035943984985\n",
      "Epoch 25/50, Week 10, Train Loss: 6.234146627626921, Validation Loss: 6.704768876234691\n",
      "Epoch 26/50, Week 10, Train Loss: 6.223626618636282, Validation Loss: 6.676292777061462\n",
      "Epoch 27/50, Week 10, Train Loss: 6.226380147432026, Validation Loss: 6.659927070140839\n",
      "Epoch 28/50, Week 10, Train Loss: 6.227833692651046, Validation Loss: 6.669561584790547\n",
      "Epoch 29/50, Week 10, Train Loss: 6.226050171099211, Validation Loss: 6.670852879683177\n",
      "Epoch 30/50, Week 10, Train Loss: 6.226270023145173, Validation Loss: 6.675178507963817\n",
      "Epoch 31/50, Week 10, Train Loss: 6.232354470303184, Validation Loss: 6.668366372585297\n",
      "Epoch 32/50, Week 10, Train Loss: 6.229052237460488, Validation Loss: 6.656377772490184\n",
      "Epoch 33/50, Week 10, Train Loss: 6.235890032115735, Validation Loss: 6.6881090203921\n",
      "Epoch 34/50, Week 10, Train Loss: 6.228717334646928, Validation Loss: 6.673249264558156\n",
      "Epoch 35/50, Week 10, Train Loss: 6.231273199382581, Validation Loss: 6.665070176124573\n",
      "Epoch 36/50, Week 10, Train Loss: 6.234826946258545, Validation Loss: 6.6682384212811785\n",
      "Epoch 37/50, Week 10, Train Loss: 6.230808150140863, Validation Loss: 6.683704276879628\n",
      "Epoch 38/50, Week 10, Train Loss: 6.232300806045532, Validation Loss: 6.692148009936015\n",
      "Epoch 39/50, Week 10, Train Loss: 6.234776770441155, Validation Loss: 6.701823651790619\n",
      "Epoch 40/50, Week 10, Train Loss: 6.233197681527389, Validation Loss: 6.663232823212941\n",
      "Epoch 41/50, Week 10, Train Loss: 6.234891474874396, Validation Loss: 6.649796485900879\n",
      "Epoch 42/50, Week 10, Train Loss: 6.226075573971397, Validation Loss: 6.6897212266922\n",
      "Epoch 43/50, Week 10, Train Loss: 6.2362035650956, Validation Loss: 6.641438831885655\n",
      "Epoch 44/50, Week 10, Train Loss: 6.2298563430183815, Validation Loss: 6.6659441987673445\n",
      "Epoch 45/50, Week 10, Train Loss: 6.225086528376529, Validation Loss: 6.681239108244578\n",
      "Epoch 46/50, Week 10, Train Loss: 6.236683421385916, Validation Loss: 6.666918198267619\n",
      "Epoch 47/50, Week 10, Train Loss: 6.232909373233193, Validation Loss: 6.658604919910431\n",
      "Epoch 48/50, Week 10, Train Loss: 6.23661968331588, Validation Loss: 6.667699019114177\n",
      "Epoch 49/50, Week 10, Train Loss: 6.233130028373317, Validation Loss: 6.644397060076396\n",
      "Epoch 50/50, Week 10, Train Loss: 6.229291569559198, Validation Loss: 6.6565021475156145\n"
     ]
    }
   ],
   "source": [
    "rates = [(1,50),(10,50),(14,20),(20,10),(25,5),(14,20),(10,50)]\n",
    "for week, epochs in rates:\n",
    "    train_kilo_model(k_model,train_loader,val_loader,criterion,k_opt,k_scheduler,epochs,week)\n",
    "    train_schedule_model(s_model,train_loader,val_loader,criterion,s_opt,s_scheduler,epochs,week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(k_model.state_dict(), 'k_model.pth')\n",
    "torch.save(s_model.state_dict(), 's_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cherry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
